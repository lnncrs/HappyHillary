{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nesse notebook vamos aprender a usar a biblioteca `ml`.\n",
    "\n",
    "### Como primeiro passo, vamos carregar a base de dados Tweets, obtida no [Kaggle](http://kaggle.com) e verificar a estrutura dela, pelo cabeçalho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "airlineRDD = sc.textFile('Tweets.csv')\n",
    "\n",
    "header = airlineRDD.take(1)[0]\n",
    "print header.split(',')\n",
    "\n",
    "airlineDataRDD = (airlineRDD\n",
    "                  .filter(lambda x: x!=header)\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Em seguida, vamos extrair apenas os campos de interesse: o campo \\#1 que contém o sentimento e o campo \\#10 que contém o texto.\n",
    "\n",
    "### Para garantir compatibilidade precisamos fazer para cada linha:\n",
    "* Codificar o texto em utf-8\n",
    "* Dividir a linha em campos, com um parser de csv\n",
    "* filtrar linhas que contém um número inválido de campos (<11)\n",
    "* criar uma RDD de tuplas no formato (sentimento, texto), com sentimento transformado em 0 ou 1\n",
    "* eliminar sentimentos inválidos\n",
    "* eliminar lihas sem texto\n",
    "* eliminar tuplas inválidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def csvParse(s):\n",
    "    import csv\n",
    "    from StringIO import StringIO\n",
    "    sio = StringIO(s)\n",
    "    value = csv.reader(sio).next()\n",
    "    sio.close()\n",
    "    return value\n",
    "\n",
    "def Sent2Id(sent):\n",
    "    if sent == 'negative':\n",
    "        return 0.0\n",
    "    elif sent == 'positive':\n",
    "        return 1.0\n",
    "    else:\n",
    "        return -1.0\n",
    "\n",
    "sentimentLocRDD = (airlineDataRDD\n",
    "                   .map(lambda line: line.encode('utf-8'))\n",
    "                   .map(csvParse)\n",
    "                   .filter(lambda x: len(x) >= 11)\n",
    "                   .map(lambda x: (Sent2Id(x[1]),x[10]))\n",
    "                   .filter(lambda x: x[0]>=0)\n",
    "                   .filter(lambda x: len(x[1]))\n",
    "                   .filter(lambda x: len(x)==2)\n",
    "                  )\n",
    "\n",
    "sentimentLocRDD.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A biblioteca `ml` trabalha com DataFrames (que no futuro se tornarão DataSet), que são bases de dados estruturadas.\n",
    "\n",
    "### Vamos criar nossa DataFrame com:\n",
    "\n",
    "```\n",
    "sqlContext.createDataFrame( RDD, lista de campos )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sentiment classification\n",
    "df = sqlContext.createDataFrame(sentimentLocRDD , ['label','tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Os DataFrames permitem a utilização de comandos SQL tradicionais:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labeledData = df.select(df.label, df.tokens).where('label >= 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finalmente, utilizaremos o `Pipeline` para definir a sequência de processamento de nossa RDD (que pode ser aplicado em outras RDDs de mesma estrutura).\n",
    "\n",
    "### Nosso Pipeline consistirá de:\n",
    "\n",
    "* Tokenizar o texto com `Tokenizer`\n",
    "* Criar vetor TF com `HashingTF`\n",
    "* Aplicar um algoritmo de Regressão Logística para aprender a classificar nossos dados com `LogisticRegression`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"tokens\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(numFeatures=1000, inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "\n",
    "model = pipeline.fit(labeledData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uma vez que o modelo foi treinado, podemos aplicar esse mesmo Pipeline para predizer novos elementos com o método `transform`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictionsDF = model.transform(df)\n",
    "for row in predictionsDF.take(10):\n",
    "    print row[1].encode('utf-8'), row[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python"
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
